```python
import torch
from transformers import LlamaTokenizer
from lora_model import LLaMAWithLoRA

# Check for CUDA
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Load model and tokenizer
model_name = "llama-base"
model = LLaMAWithLoRA(model_name).to(device)
tokenizer = LlamaTokenizer.from_pretrained(model_name)

# Define training function

def train(model, tokenizer, device, texts, epochs=1):
    model.train()
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)

    for epoch in range(epochs):
        for text in texts:
            inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True).to(device)
            optimizer.zero_grad()
            outputs = model(**inputs)

            # Dummy loss calculation
            loss = outputs.sum()
            loss.backward()
            optimizer.step()

            print(f"Epoch {epoch + 1}, Loss: {loss.item()}")


# Example training data
texts = ["Hello, how are you?", "What's your name?", "Tell me a joke."]

# Train the model
train(model, tokenizer, device, texts, epochs=3)
